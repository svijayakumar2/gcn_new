{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def load_results(baseline_path: str, method_path: str):\n",
    "    with open(baseline_path) as f:\n",
    "        baseline_results = json.load(f)\n",
    "    with open(method_path) as f:\n",
    "        method_results = json.load(f)\n",
    "    return baseline_results, method_results\n",
    "\n",
    "def set_style():\n",
    "    plt.style.use('seaborn')\n",
    "    colors = sns.color_palette(\"husl\", 4)\n",
    "    sns.set_palette(colors)\n",
    "    return colors\n",
    "\n",
    "def plot_performance_comparison(baseline_results, method_results, colors):\n",
    "    methods_data = []\n",
    "    \n",
    "    # Baseline metrics\n",
    "    methods_data.extend([\n",
    "        {\n",
    "            'Method': 'Traditional GNN',\n",
    "            'Type': 'Baseline',\n",
    "            'Precision': baseline_results['baseline_gnn']['overall']['precision'],\n",
    "            'Recall': baseline_results['baseline_gnn']['overall']['recall'],\n",
    "            'F1': baseline_results['baseline_gnn']['overall']['f1']\n",
    "        },\n",
    "        {\n",
    "            'Method': 'Random Forest',\n",
    "            'Type': 'Baseline',\n",
    "            'Precision': baseline_results['random_forest']['overall']['precision'],\n",
    "            'Recall': baseline_results['random_forest']['overall']['recall'],\n",
    "            'F1': baseline_results['random_forest']['overall']['f1']\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    # Our method metrics\n",
    "    test_results = method_results['test_results']\n",
    "    methods_data.extend([\n",
    "        {\n",
    "            'Method': 'Family-Level',\n",
    "            'Type': 'Temporal-Symbolic Model',\n",
    "            'Precision': test_results['family']['metrics']['overall']['precision'],\n",
    "            'Recall': test_results['family']['metrics']['overall']['recall'],\n",
    "            'F1': test_results['family']['metrics']['overall']['f1']\n",
    "        },\n",
    "        {\n",
    "            'Method': 'Group-Level',\n",
    "            'Type': 'Temporal-Symbolic Model',\n",
    "            'Precision': test_results['group']['metrics']['overall']['precision'],\n",
    "            'Recall': test_results['group']['metrics']['overall']['recall'],\n",
    "            'F1': test_results['group']['metrics']['overall']['f1']\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    df = pd.DataFrame(methods_data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    bar_width = 0.25\n",
    "    opacity = 0.8\n",
    "    \n",
    "    index = np.arange(len(df['Method'].unique()))\n",
    "    \n",
    "    metrics = ['Precision', 'Recall', 'F1']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.bar(index + i*bar_width, \n",
    "                df[metric], \n",
    "                bar_width,\n",
    "                alpha=opacity,\n",
    "                color=colors[i],\n",
    "                label=metric)\n",
    "    \n",
    "    plt.xlabel('Methods')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Performance Comparison: Baselines vs Temporal-Symbolic Model')\n",
    "    plt.xticks(index + bar_width, df['Method'], rotation=45)\n",
    "    \n",
    "    # Add type labels\n",
    "    for i, method in enumerate(df['Method']):\n",
    "        type_label = df[df['Method'] == method]['Type'].iloc[0]\n",
    "        plt.text(i, -0.05, type_label, \n",
    "                rotation=45, ha='right', va='top', \n",
    "                transform=ax.get_xaxis_transform())\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_novel_detection(baseline_results, method_results, colors):\n",
    "    methods_data = []\n",
    "    \n",
    "    # Baseline novel detection\n",
    "    for method in ['isolation_forest', 'one_class_svm']:\n",
    "        methods_data.append({\n",
    "            'Method': method.replace('_', ' ').title(),\n",
    "            'Type': 'Baseline',\n",
    "            'Precision': baseline_results[method]['precision'],\n",
    "            'Recall': baseline_results[method]['recall'],\n",
    "            'F1': baseline_results[method]['f1']\n",
    "        })\n",
    "    \n",
    "    # Our method novel detection\n",
    "    test_results = method_results['test_results']\n",
    "    for model in ['family', 'group']:\n",
    "        methods_data.append({\n",
    "            'Method': f'{model.title()}-Level',\n",
    "            'Type': 'Temporal-Symbolic Model',\n",
    "            'Precision': test_results[model]['novel_detection']['overall']['precision'],\n",
    "            'Recall': test_results[model]['novel_detection']['overall']['recall'],\n",
    "            'F1': test_results[model]['novel_detection']['overall']['f1']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(methods_data)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=pd.melt(df, \n",
    "                            id_vars=['Method', 'Type'], \n",
    "                            var_name='Metric', \n",
    "                            value_name='Score'),\n",
    "                x='Method', y='Score', hue='Metric', palette=colors[:3])\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Novel Detection Performance')\n",
    "    \n",
    "    # Add type labels\n",
    "    ax = plt.gca()\n",
    "    for i, method in enumerate(df['Method']):\n",
    "        type_label = df[df['Method'] == method]['Type'].iloc[0]\n",
    "        plt.text(i, -0.05, type_label, \n",
    "                rotation=45, ha='right', va='top', \n",
    "                transform=ax.get_xaxis_transform())\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('novel_detection_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def plot_training_history(method_results):\n",
    "    history = method_results['training_history']\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "    \n",
    "    for model_type in ['family', 'group']:\n",
    "        metrics = pd.DataFrame([{\n",
    "            'epoch': data['epoch'],\n",
    "            'train_acc': data['train']['accuracy'],\n",
    "            'train_loss': data['train']['loss'],\n",
    "            'val_acc': np.mean([m['recall'] for m in data['val']['per_class'].values() \n",
    "                              if isinstance(m, dict) and m['support'] > 0])\n",
    "        } for data in history[model_type]])\n",
    "        \n",
    "        ax = ax1 if model_type == 'family' else ax2\n",
    "        title = f\"{model_type.title()}-Level Training\"\n",
    "        \n",
    "        # Plot accuracy and loss\n",
    "        ax.plot(metrics['epoch'], metrics['train_acc'], label='Train Accuracy')\n",
    "        ax.plot(metrics['epoch'], metrics['val_acc'], label='Validation Accuracy')\n",
    "        ax2 = ax.twinx()  # Create second y-axis\n",
    "        ax2.plot(metrics['epoch'], metrics['train_loss'], 'r--', label='Train Loss')\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Accuracy')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_group_distribution(method_results):\n",
    "    data_stats = method_results['data_statistics']['test']['group']\n",
    "    \n",
    "    # Convert counts to DataFrame\n",
    "    df = pd.DataFrame([\n",
    "        {'group': group, 'count': count}\n",
    "        for group, count in data_stats['known_groups'].items()\n",
    "    ])\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x='group', y='count')\n",
    "    plt.title('Behavioral Group Distribution')\n",
    "    plt.xlabel('Group ID')\n",
    "    plt.ylabel('Sample Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('group_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    colors = set_style()\n",
    "\n",
    "    baseline_results, method_results = load_results('/data/saranyav/gcn_new/baseline_results.json', '/data/saranyav/gcn_new/final_report.json')\n",
    "    # Generate all plots\n",
    "    plot_performance_comparison(baseline_results, method_results, colors)\n",
    "    plot_novel_detection(baseline_results, method_results, colors)\n",
    "    plot_training_history(method_results)\n",
    "    plot_group_distribution(method_results)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
